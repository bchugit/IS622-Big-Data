{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS622 Week 14 Discussion\n",
    "## Counting triangles with PySpark / MapReduce\n",
    "### Brian Chu | Nov. 29, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 14 2015 16:09:02)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for Spark source folder\n",
    "os.environ['SPARK_HOME']=\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/python/\")\n",
    "\n",
    "# Append py4j to Python Path\n",
    "sys.path.append(\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\")\n",
    "\n",
    "# Launch Spark\n",
    "execfile(\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/python/pyspark/shell.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define edges and make copy for upcoming join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure 10-1 example\n",
    "e1 = [('A','B'), ('A','C'), ('B','C'), ('B','D'), ('D','E'), ('D','F'), ('D','G'), ('E','F'), ('G','F')]\n",
    "e1dist = sc.parallelize(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to PySpark RDDs\n",
    "e1df = sqlc.createDataFrame(e1dist)\n",
    "e1rdd = e1df.rdd\n",
    "e2rdd = e1rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define node key to join edge pairs\n",
    "Some theory and source code borrowed or modified from:  \n",
    "https://class.coursera.org/datasci-001/lecture/85  \n",
    "http://dataorigami.net/blogs/napkin-folding/18433407-joins-in-mapreduce-pt-2-generalizing-joins  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e1key = e1rdd.keyBy(lambda r: r[0])\n",
    "e2key = e2rdd.keyBy(lambda r: r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', Row(_1=u'A', _2=u'B')),\n",
       " (u'A', Row(_1=u'A', _2=u'C')),\n",
       " (u'B', Row(_1=u'B', _2=u'C')),\n",
       " (u'B', Row(_1=u'B', _2=u'D')),\n",
       " (u'D', Row(_1=u'D', _2=u'E')),\n",
       " (u'D', Row(_1=u'D', _2=u'F')),\n",
       " (u'D', Row(_1=u'D', _2=u'G')),\n",
       " (u'E', Row(_1=u'E', _2=u'F')),\n",
       " (u'G', Row(_1=u'G', _2=u'F'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine values\n",
    "e1key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B', Row(_1=u'A', _2=u'B')),\n",
       " (u'C', Row(_1=u'A', _2=u'C')),\n",
       " (u'C', Row(_1=u'B', _2=u'C')),\n",
       " (u'D', Row(_1=u'B', _2=u'D')),\n",
       " (u'E', Row(_1=u'D', _2=u'E')),\n",
       " (u'F', Row(_1=u'D', _2=u'F')),\n",
       " (u'G', Row(_1=u'D', _2=u'G')),\n",
       " (u'F', Row(_1=u'E', _2=u'F')),\n",
       " (u'F', Row(_1=u'G', _2=u'F'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine values\n",
    "e2key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map tuples with addition x/y value to determine which schema the edge is from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e1map = e1key.map(lambda (k,v): (k, ('x',v)))\n",
    "e2map = e2key.map(lambda (k,v): (k, ('y',v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', ('x', Row(_1=u'A', _2=u'B'))),\n",
       " (u'A', ('x', Row(_1=u'A', _2=u'C'))),\n",
       " (u'B', ('x', Row(_1=u'B', _2=u'C'))),\n",
       " (u'B', ('x', Row(_1=u'B', _2=u'D'))),\n",
       " (u'D', ('x', Row(_1=u'D', _2=u'E'))),\n",
       " (u'D', ('x', Row(_1=u'D', _2=u'F'))),\n",
       " (u'D', ('x', Row(_1=u'D', _2=u'G'))),\n",
       " (u'E', ('x', Row(_1=u'E', _2=u'F'))),\n",
       " (u'G', ('x', Row(_1=u'G', _2=u'F')))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B', ('y', Row(_1=u'A', _2=u'B'))),\n",
       " (u'C', ('y', Row(_1=u'A', _2=u'C'))),\n",
       " (u'C', ('y', Row(_1=u'B', _2=u'C'))),\n",
       " (u'D', ('y', Row(_1=u'B', _2=u'D'))),\n",
       " (u'E', ('y', Row(_1=u'D', _2=u'E'))),\n",
       " (u'F', ('y', Row(_1=u'D', _2=u'F'))),\n",
       " (u'G', ('y', Row(_1=u'D', _2=u'G'))),\n",
       " (u'F', ('y', Row(_1=u'E', _2=u'F'))),\n",
       " (u'F', ('y', Row(_1=u'G', _2=u'F')))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all tuples and group by the node key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', ('x', Row(_1=u'A', _2=u'B'))),\n",
       " (u'A', ('x', Row(_1=u'A', _2=u'C'))),\n",
       " (u'B', ('x', Row(_1=u'B', _2=u'C'))),\n",
       " (u'B', ('x', Row(_1=u'B', _2=u'D'))),\n",
       " (u'D', ('x', Row(_1=u'D', _2=u'E'))),\n",
       " (u'D', ('x', Row(_1=u'D', _2=u'F'))),\n",
       " (u'D', ('x', Row(_1=u'D', _2=u'G'))),\n",
       " (u'E', ('x', Row(_1=u'E', _2=u'F'))),\n",
       " (u'G', ('x', Row(_1=u'G', _2=u'F'))),\n",
       " (u'B', ('y', Row(_1=u'A', _2=u'B'))),\n",
       " (u'C', ('y', Row(_1=u'A', _2=u'C'))),\n",
       " (u'C', ('y', Row(_1=u'B', _2=u'C'))),\n",
       " (u'D', ('y', Row(_1=u'B', _2=u'D'))),\n",
       " (u'E', ('y', Row(_1=u'D', _2=u'E'))),\n",
       " (u'F', ('y', Row(_1=u'D', _2=u'F'))),\n",
       " (u'G', ('y', Row(_1=u'D', _2=u'G'))),\n",
       " (u'F', ('y', Row(_1=u'E', _2=u'F'))),\n",
       " (u'F', ('y', Row(_1=u'G', _2=u'F')))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unioned = e1map.union(e2map)\n",
    "unioned.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', <pyspark.resultiterable.ResultIterable at 0x7fa0fac5e590>),\n",
       " (u'C', <pyspark.resultiterable.ResultIterable at 0x7fa0fabf0290>),\n",
       " (u'E', <pyspark.resultiterable.ResultIterable at 0x7fa0fabf0690>),\n",
       " (u'G', <pyspark.resultiterable.ResultIterable at 0x7fa0fabf0710>),\n",
       " (u'B', <pyspark.resultiterable.ResultIterable at 0x7fa0fabf0750>),\n",
       " (u'D', <pyspark.resultiterable.ResultIterable at 0x7fa0fabf0790>),\n",
       " (u'F', <pyspark.resultiterable.ResultIterable at 0x7fa0fabf07d0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = unioned.groupByKey()\n",
    "grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join function for node keys with common edge values\n",
    "#### This produces the relation e1.B = e2.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def joined(seq):\n",
    "    x_keys = []\n",
    "    y_keys = []\n",
    "    for (n, v) in seq:\n",
    "        if n == 'x':\n",
    "            x_keys.append(v)\n",
    "        elif n == 'y':\n",
    "            y_keys.append(v)\n",
    "    return [(y, x) for x in x_keys for y in y_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B', (Row(_1=u'A', _2=u'B'), Row(_1=u'B', _2=u'C'))),\n",
       " (u'B', (Row(_1=u'A', _2=u'B'), Row(_1=u'B', _2=u'D'))),\n",
       " (u'D', (Row(_1=u'B', _2=u'D'), Row(_1=u'D', _2=u'E'))),\n",
       " (u'D', (Row(_1=u'B', _2=u'D'), Row(_1=u'D', _2=u'F'))),\n",
       " (u'D', (Row(_1=u'B', _2=u'D'), Row(_1=u'D', _2=u'G'))),\n",
       " (u'E', (Row(_1=u'D', _2=u'E'), Row(_1=u'E', _2=u'F'))),\n",
       " (u'G', (Row(_1=u'D', _2=u'G'), Row(_1=u'G', _2=u'F')))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = grouped.flatMapValues(lambda x : joined(x))\n",
    "sorted(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter (reduce) out pairs that match remaining edge\n",
    "#### This produces the relations e1.B = e3.A and e2.B = e3.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B', (Row(_1=u'A', _2=u'B'), Row(_1=u'B', _2=u'C'))),\n",
       " (u'E', (Row(_1=u'D', _2=u'E'), Row(_1=u'E', _2=u'F'))),\n",
       " (u'G', (Row(_1=u'D', _2=u'G'), Row(_1=u'G', _2=u'F')))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3 = e1\n",
    "matching_edges = result.filter(lambda x: (x[1][0][0], x[1][1][1]) in e3).collect()\n",
    "sorted(matching_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print resulting triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A-B-C', 'D-E-F', 'D-G-F']\n"
     ]
    }
   ],
   "source": [
    "tris = sorted([(x[1][0][0], x[1][0][1], x[1][1][1]) for x in matching_edges])\n",
    "print [str('-'.join(x)) for x in tris]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all of the above into one function and to test other graphs and evaluate runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input list of edges (as tuples)\n",
    "def triangles(edges1):\n",
    "    \n",
    "    edges1dist = sc.parallelize(edges1)\n",
    "    edges1df = sqlc.createDataFrame(edges1dist)\n",
    "    edges1rdd = edges1df.rdd\n",
    "    edges2rdd = edges1rdd\n",
    "\n",
    "    edges1key = edges1rdd.keyBy(lambda r: r[0])\n",
    "    edges2key = edges2rdd.keyBy(lambda r: r[1])\n",
    "    \n",
    "    edges1map = edges1key.map(lambda (k,v): (k, ('x',v)))\n",
    "    edges2map = edges2key.map(lambda (k,v): (k, ('y',v)))\n",
    "\n",
    "    unioned = edges1map.union(edges2map)\n",
    "    grouped = unioned.groupByKey()\n",
    "    \n",
    "    result = grouped.flatMapValues(lambda x : joined(x))\n",
    "    edges3 = edges1\n",
    "    matching_edges = result.filter(lambda x: (x[1][0][0], x[1][1][1]) in edges3).collect()\n",
    "    \n",
    "    tris = sorted([(x[1][0][0], x[1][0][1], x[1][1][1]) for x in matching_edges])\n",
    "    print [str('-'.join(t)) for t in tris]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A-B-C', 'D-E-F', 'D-G-F']\n"
     ]
    }
   ],
   "source": [
    "fig10_1 = [('A','B'), ('A','C'), ('B','C'), ('B','D'), ('D','E'), ('D','F'), ('D','G'), ('E','F'), ('G','F')]\n",
    "triangles(fig10_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 5 loops, best of 3: 726 ms per loop>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "%timeit -n 5 -o triangles(fig10_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A-B-C', 'D-E-F', 'G-H-I']\n"
     ]
    }
   ],
   "source": [
    "fig10_9 = [('A','B'), ('A','C'), ('B','C'), ('B','H'), ('C','D'), ('D','E'), ('D','F'),\n",
    "           ('E','F'), ('G','H'), ('G','I'), ('H','I')]\n",
    "triangles(fig10_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 5 loops, best of 3: 664 ms per loop>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "%timeit -n 5 -o triangles(fig10_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U1-T1-W1', 'U1-T2-W1', 'U1-T2-W2', 'U1-T3-W2', 'U2-T2-W2', 'U2-T2-W3', 'U2-T3-W2', 'U2-T4-W2', 'U2-T4-W3']\n"
     ]
    }
   ],
   "source": [
    "fig10_2 = [('U1','T1'), ('U1','T2'), ('U1','T3'), ('U1','W1'), ('U1','W2'),\n",
    "           ('U2','T2'), ('U2','T3'), ('U2','T4'), ('U2','W2'), ('U2','W3'),\n",
    "           ('T1','W1'), ('T2','W1'), ('T2','W2'), ('T2','W3'),\n",
    "           ('T3','W2'), ('T4','W2'), ('T4','W3')]\n",
    "\n",
    "triangles(fig10_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 5 loops, best of 3: 710 ms per loop>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "%timeit -n 5 -o triangles(fig10_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feels like a fairly clunky and inefficient function with excessive maps and joins. Sure enough, my R function did the triangle counts in roughly 2-3 ms, which is significantly faster than the PySpark function above. However, with larger graphs, PySpark distributed clusters may be more favorable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
