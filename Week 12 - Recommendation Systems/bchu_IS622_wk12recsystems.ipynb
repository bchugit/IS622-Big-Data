{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS622 Recommendation System Mini-Project\n",
    "### Brian Chu | Nov 15, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for Spark source folder\n",
    "os.environ['SPARK_HOME']=\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6\"\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/python/\")\n",
    "\n",
    "# Append py4j to Python Path\n",
    "sys.path.append(\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 14 2015 16:09:02)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "# Launch Spark\n",
    "execfile(\"/home/brian/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/python/pyspark/shell.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "from pyspark.sql import SQLContext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Jester Online Joke Recommender System\n",
    "**http://eigentaste.berkeley.edu/dataset/**  \n",
    "Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.  \n",
    "  \n",
    "This dataset includes ratings from [-10, 10] of up to 100 jokes by 24,983 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,-7.82,8.79,-9.66,-8.16,-7.52,-8.5,-9.85,4.17,-8.98,-4.76,-8.5,-6.75,-7.18,8.45,-7.18,-7.52,-7.43,-9.81,-9.85,-9.85,-9.37,1.5,-4.37,-9.81,-8.5,1.12,7.82,2.86,9.13,-7.43,2.14,-4.08,-9.08,7.82,5.05,4.95,-9.17,-8.4,-8.4,-8.4,-8.11,-9.13,-9.03,-9.08,-7.14,-6.26,3.79,-0.1,3.93,4.13,-8.69,-7.14,3.2,8.3,-4.56,0.92,-9.13,-9.42,2.82,-8.64,8.59,3.59,-6.84,-9.03,2.82,-1.36,-9.08,8.3,5.68,-4.81,99,99,99,99,99,99,99,-9.42,99,99,99,-7.72,99,99,99,99,99,99,99,99,2.82,99,99,99,99,99,-5.63,99,99,99']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jester = sc.textFile(\"jester2.csv\")\n",
    "\n",
    "# Sample row\n",
    "jester.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create RDD, DataFrame, and Pandas objects\n",
    "jrdd = jester.map(lambda line: line.split(\",\"))\n",
    "jdf = jrdd.toDF()\n",
    "jpd = jdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode missing data (99) to NaN. I found this easier to do in pandas and then reconvert to PySpark dataframe. Perhaps because dataframes are immutable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert 99 to NaN (in pandas, remake DF)\n",
    "sqlc = SQLContext(sc)\n",
    "jpd.iloc[:,1:] = jpd.iloc[:,1:].astype(float)\n",
    "jpd[jpd==99] = np.nan\n",
    "jdf = sqlc.createDataFrame(jpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark MLlib  \n",
    "I am going to use PySpark's MLlib package, which incorporates collaborative filtering using the Alternating Least Squares (ALS) algorithm to predict missing entries. ALS aims to minimize the squared error of the UV user-item rating matrix by alternating learning sequences of U to V and V to U until a steady-state has been reached. This is similar to the collaborative filtering theory discussed in the MMS text.  \n",
    "  \n",
    "### Modified source code:  \n",
    "http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html  \n",
    "https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLlib ALS requires a Ratings object, which is a tuple in the format (userID, itemID, itemRating)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Ratings object required for MLlib ALS train function\n",
    "# Exclude NaN entries for model training; will be used for testing and predicting\n",
    "ratings = jdf.flatMap(lambda line: [(line[0], i, line[i]) for i in range(1,101) if np.isnan(line[i])==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', 1, -7.82),\n",
       " (u'1', 2, 8.79),\n",
       " (u'1', 3, -9.66),\n",
       " (u'1', 4, -8.16),\n",
       " (u'1', 5, -7.52)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View snippet of Ratings object\n",
    "ratings.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the number of users, jokes, and ratings in our dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1810455 ratings from 24983 users on 100 jokes.\n"
     ]
    }
   ],
   "source": [
    "numRatings = ratings.count()\n",
    "numUsers = ratings.map(lambda r: r[0]).distinct().count()\n",
    "numJokes = ratings.map(lambda r: r[1]).distinct().count()\n",
    "\n",
    "print \"Got %d ratings from %d users on %d jokes.\" % (numRatings, numUsers, numJokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly divide ratings into a 75% training and 25% testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingsTrain, ratingsTest = ratings.randomSplit([0.75, 0.25], seed = 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357773\n",
      "452682\n"
     ]
    }
   ],
   "source": [
    "# Check number of ratings in each set\n",
    "print len(ratingsTrain.collect())\n",
    "print len(ratingsTest.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the recommendation model using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "# Default parameters; can also tune using function defined in MLlib example\n",
    "rank = 5\n",
    "numIterations = 10\n",
    "\n",
    "# Train model with training data\n",
    "model = ALS.train(ratingsTrain, rank, numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check model accuracy and error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rt = sc.parallelize(ratingsTest.collect())\n",
    "test = model.predictAll(rt.map(lambda p: (p[0], p[1]))).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "results = ratingsTest.map(lambda r: ((int(r[0]), r[1]), r[2])).join(test)\n",
    "rpar = sc.parallelize(results.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 17.767\n",
      "Root Mean Squared Error = 4.215 \n",
      "\n",
      "Mean Absolute Error = 3.295\n",
      "Normalized Mean Abolute Error = 0.165\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "MSE = rpar.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print \"Mean Squared Error = %.3f\" %(MSE)\n",
    "\n",
    "RMSE = math.sqrt(MSE)\n",
    "print \"Root Mean Squared Error = %.3f \\n\" %(RMSE)\n",
    "\n",
    "MAE = rpar.map(lambda r: abs((r[1][0] - r[1][1]))).mean()\n",
    "print \"Mean Absolute Error = %.3f\" %(MAE)\n",
    "\n",
    "NMAE = MAE / (10 - -10)\n",
    "print \"Normalized Mean Abolute Error = %.3f\" %(NMAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tinkered with some of the tuning parameters (rank, iterations) and found the RMSE to be fairly consistent. The ratings scale goes from -10 to 10 (range of 20) so both RMSE and MAE may seem somewhat large. However, in the cited paper above, Goldberg et al. looked at a few other algorithms and found the NMAE to range from 0.187 to 0.237. Relatively, the ALS model here would appear very good. Note that NMAE was preferred by the authors to normalize errors as a percentage of the full scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make recommendations using model and set of jokes not rated\n",
    "*Example: User 13*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify user\n",
    "user = '13'\n",
    "\n",
    "# List of all 100 Joke IDs\n",
    "jokeID = list(range(1,101))\n",
    "\n",
    "# User's rated jokes\n",
    "uRatings = [(j,r) for (u,j,r) in ratings.collect() if u==user]\n",
    "uRatingsID = [j[0] for j in uRatings]\n",
    "\n",
    "# Check number of jokes rated\n",
    "len(uRatingsID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jokes not rated by User\n",
    "uNR = [j for j in jokeID if j not in uRatingsID]\n",
    "len(uNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notRated = sc.parallelize(set(uNR))\n",
    "predictions = model.predictAll(notRated.map(lambda x: (user, x))).collect()\n",
    "\n",
    "# Sort by highest rated\n",
    "predictions = sorted(predictions, key=lambda k: k[2], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only recommend joke IDs that are positive and above the user's average rating**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended jokes (score out of 10): \n",
      "\n",
      "JokeID: 89    Score: 7.04\n",
      "JokeID: 72    Score: 6.94\n",
      "JokeID: 12    Score: 6.88\n",
      "JokeID: 26    Score: 6.81\n",
      "JokeID: 6    Score: 6.41\n",
      "JokeID: 81    Score: 6.19\n",
      "JokeID: 83    Score: 6.11\n",
      "JokeID: 76    Score: 6.10\n",
      "JokeID: 100    Score: 5.96\n",
      "JokeID: 34    Score: 5.88\n",
      "JokeID: 40    Score: 5.71\n",
      "JokeID: 22    Score: 5.66\n",
      "JokeID: 87    Score: 5.44\n",
      "JokeID: 88    Score: 5.35\n",
      "JokeID: 80    Score: 5.31\n",
      "JokeID: 78    Score: 5.28\n"
     ]
    }
   ],
   "source": [
    "avgRating = np.mean([x[1] for x in uRatings])\n",
    "highRecommend = [(x[1], x[2]) for x in predictions if x[2] >= avgRating and x[2] >= 0]\n",
    "\n",
    "print \"Recommended jokes (score out of 10): \\n\"\n",
    "for i in highRecommend:\n",
    "    print \"JokeID: %d    Score: %.2f\" %(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print full joke of the top 5 recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Joke\n",
      "A radio conversation of a US naval \n",
      "ship with Canadian authorities ...\n",
      "Americans: Please divert your course 15 degrees to the North to avoid a\n",
      "collision.\n",
      "Canadians: Recommend you divert YOUR course 15 degrees to the South to \n",
      "avoid a collision.\n",
      "Americans: This is the Captain of a US Navy ship.  I say again, divert \n",
      "YOUR course.\n",
      "Canadians: No.  I say again, you divert YOUR course.\n",
      "Americans: This is the aircraft carrier USS LINCOLN, the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers, three cruisers and numerous support vessels. I demand that you change your course 15 degrees north, that's ONE FIVE DEGREES NORTH, or counter-measures will be undertaken to ensure the safety of this ship.\n",
      "Canadians:\n",
      "This is a lighthouse.  Your call\n",
      ".\n",
      "\n",
      "\n",
      "A Joke\n",
      "On the first day of college, the Dean addressed the students,\n",
      "pointing out some of the rules:\n",
      "\n",
      "\"The female dormitory will be out-of-bounds for all male students\n",
      "and the male dormitory to the female students. Anybody caught breaking\n",
      "this rule will be finded $20 the first time.\" He continued, \"Anybody \n",
      "caught breaking this rule the second time will be fined $60. Being caught\n",
      "a third time will cost you a fine of $180. Are there any questions ?\"\n",
      "At this point, a male student in the crowd inquired:\n",
      "\"How much for a season pass ?\"\n",
      "\n",
      "\n",
      "Joke 12 of 20\n",
      "A guy stood over his tee shot for what seemed an eternity, looking up, looking down, measuring the distance,\n",
      "figuring the wind direction and speed. Driving his partner nuts.\n",
      "Finally his exasperated partner says, \"What the hell is taking so long? Hit the goddamn ball!\"\n",
      "The guy answers, \"My wife is up there watching me from the clubhouse. I want to make this a perfect shot.\"\n",
      "\"Well, hell, man, you don't stand a snowball's chance in hell of hitting her from here!\"\n",
      "\n",
      "\n",
      "Joke 26\n",
      "A guy walks into a bar and sits down next to an extremely gorgeous \n",
      "woman.  The first thing he notices about her though, are her pants.  \n",
      "They were skin-tight, high-waisted and had no obvious mechanism \n",
      "(zipper, buttons or velcro) for opening them.\n",
      "After several minutes of puzzling over how she got the pants up over \n",
      "her hips, he finally worked up the nerve to ask her.  \"Excuse me miss, \n",
      "but how do you get into your pants?\"\n",
      "\"Well,\" she replied, \"you can start by buying me a drink.\"\n",
      "\n",
      "\n",
      "Joke 6\n",
      "Bill & Hillary are on a trip back to Arkansas. They're almost out of gas, so Bill pulls into a service station on the outskirts of\n",
      "town. The attendant runs out of the station to serve them when Hillary realizes it's an old boyfriend from high school. She and\n",
      "the attendant chat as he gases up their car and cleans the windows. Then they all say good-bye.\n",
      "As Bill pulls the car onto the road, he turns to Hillary and says, 'Now aren't you glad you married me and not him ? You could've\n",
      "been the wife of a grease monkey !'\n",
      "To which Hillary replied, 'No, Bill. If I would have married him, you'd be pumping gas and he would be the President !'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def print_joke(jnum):\n",
    "    cdir = str(os.getcwd())\n",
    "    jdir = \"/jokes/\"\n",
    "    jpath = cdir + jdir + \"init\" + str(jnum) +\".html\" \n",
    "    html = urlopen(jpath).read()\n",
    "    text = BeautifulSoup(html).get_text(\"\\n\", strip=True)\n",
    "    return text\n",
    "\n",
    "for n in highRecommend[:5]:\n",
    "    print print_joke(n[0]) + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is interesting because while it does not explicitly account for specific content features about the joke itself (e.g. humour type, general topic), it is able to implicitly deduce such latent factors and be effective at recommending similar items. In this example for User 13, it would appear s/he likes lowbrow humour and relationship topics. The downside of collaborative filtering may be that users must rate their tastes consistently but sometimes this is harder to formalize when the 'item' is more abstract like humour. Additionally, if you equally like many genres or content features and rate several items very similarly, ALS and collaborative filtering may be less successful in its recommendations. This would be true with almost any algorithm though.  \n",
    "\n",
    "In the Goldberg paper, the authors assessed a few recommendation methods including k-nearest neighbors, global mean, and their proposed Eigentaste algorithm. The latter involves normalizing the ratings and applying principal component analysis (PCA), amongst other steps. They deduced Eigentaste was more appropriate for sparse matrices and 'capturing tastes with finer granularity'. As Eigentaste is effectively a form of collaborative filtering, it should not be too surprising that the ALS method used in this mini-project appears fairly consistent in model error and results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
