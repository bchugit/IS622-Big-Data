---
title: "IS622 Week 5 HW"
author: "Brian Chu | Sept 27, 2015"
output: html_document
---

****

####Exercise 4.2.1 : Suppose we have a stream of tuples with the schema Grades(university, courseID, studentID, grade) Assume universities are unique, but a courseID is unique only within a university (i.e., different universities may have different courses with the same ID, e.g., “CS101”) and likewise, studentID’s are unique only within a university (different universities may assign the same ID to different students). Suppose we want to answer certain queries approximately from a 1/20th sample of the data. For each of the queries below, indicate how you would construct the sample. That is, tell what the key attributes should be.  
  
**(a) For each university, estimate the average number of students in a course.**  
    
Key attribute = university  
    
**(b) Estimate the fraction of students who have a GPA of 3.5 or more.**  
    
Key attribute = studentID grouped by university (since studentID is not unique)   
  
**(c) Estimate the fraction of courses where at least half the students got “A.”**  
    
Key attribute = courseID grouped by university (since courseID is not unique)       
  
****
  
####Exercise 4.3.3: As a function of n, the number of bits and m the number of members in the set S, what number of hash functions minimizes the false-positive rate?  
  
Let r = ratio of m members to n bits = $\frac{m}{n}$  
Let y = false-positive rate  
Let k = number of hash functions  
  
Therefore, we want to minimize $F(y) = (1-e^{-rk})^{k}$ as a function of $r$ and $k$  
  
The derivative is $F'(y) = (1-e^{-rk})^{k} \cdot (ln(1-e^{-rk}) + \frac{rke^{-rk}}{1-e^{-rk}})$  
  
We set F'(y) equal to 0 and solve for k to find the number of hash functions that minimizes the false-positive rate. This is rather computationally difficult so I demonstrate by subbing in values of k between 0 and 10 into the original function until a minimum is reached. I will use the values $m=1$, $n=8$, and $r=\frac{m}{n}=\frac{1}{8}$. I am also assuming no decimal number of hash functions is possible.  

```{r}
m <- 1
n <- 8
r <- m/n

for (k in 0:10) {
  y <- (1 - exp(-k*r))^k
  print(paste("k=",k, "y=", round(y,4)))
}
```

Therefore, about 6 hash functions minimizes the error rate when r = $\frac{1}{8}$.  
  
Let's substitute values back into our derivative function and see if the slope indeed reaches 0 around the range 5-7.  

```{r}
kk <- seq(5,7,.1)
for (k in kk) {
  y <- ((1 - exp(-r*k))^k) * ((log(1 - exp(-r*k))) + (r*k*exp(-r*k) / (1-exp(-r*k))))
  print(paste("k=",k, "y=", round(y,4)))
}
```

We do in fact find that the slope reaches 0 in this range, between 5.5 and 5.6 to be exact.  
  
Also, because y gets smaller as r gets larger, we can deduce the number of k hash functions needed gets smaller as r gets larger too.  
  
****
  
####Exercise 4.5.3: Suppose we are given the stream of Exercise 4.5.1, to which we apply the Alon-Matias-Szegedy Algorithm to estimate the surprise number. For each possible value of i, if Xi is a variable starting position i, what is the value of Xi.value?  
  
```{r}
stream <- c(3, 1, 4, 1, 3, 4, 2, 1, 2)
n <- length(stream)

element <- integer(length(stream))
value <- integer(length(stream))

for (i in 1:n) {
  element[i] <- stream[i]
  value[i] <- sum(stream[i:n] == element[i])
}

df <- data.frame(element, value)
df
```

The estimate of the surprise (second) moment is given by the formula $\sum^{n}_{i=1}\frac{n(2 \cdot X.value - 1)}{n}$  
  
```{r}
e <- integer(n)

for (i in 1:nrow(df)) {
  e[i] <- n*(2 * value[i] - 1)
}

second_order_moment <- sum(e)/n
print(second_order_moment)
```

The estimate is 21. 